# PPO configuration for Connect Four
# Two-player board game with self-play against random opponent

# Environment
env = "connect_four"
num_envs = 32
num_steps = 32

# PPO hyperparameters
learning_rate = 0.0003   # Standard PPO default
lr_anneal = true         # Decay learning rate over training
gamma = 0.99             # Discount factor
gae_lambda = 0.95        # GAE smoothing
clip_epsilon = 0.1       # PPO clip range
clip_value = true        # Value function clipping
entropy_coef = 0.01      # Standard PPO default (ICLR blog recommends constant entropy)
entropy_anneal = false   # Keep constant - aggressive decay causes premature convergence
value_coef = 0.5         # Value loss weight
max_grad_norm = 0.5      # Gradient clipping
target_kl = 0.02         # KL early stopping
normalize_obs = false    # Board state doesn't need normalization

# Training - board games need more samples
total_timesteps = 2_000_000
num_epochs = 4
num_minibatches = 4
adam_epsilon = 1e-5

# Network - larger for more complex game
hidden_size = 128
num_hidden = 3
activation = "relu"

# Checkpointing
run_dir = "runs"
checkpoint_freq = 50_000

# Logging
log_freq = 5_000

# Experiment
seed = 42
