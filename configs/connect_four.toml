# PPO configuration for Connect Four
# Two-player board game with self-play against random opponent

# Environment
env = "connect_four"
num_envs = 32
num_steps = 64

# PPO hyperparameters
learning_rate = 0.0003   # Standard PPO default
lr_anneal = false        # Decay learning rate over training
gamma = 0.99             # Discount factor
gae_lambda = 0.95        # GAE smoothing
clip_epsilon = 0.1       # PPO clip range
clip_value = false       # Value function clipping disabled
entropy_coef = 0.02      # Recommended PPO default that works pretty well.
entropy_anneal = false   # Keep constant - aggressive decay causes premature convergence

# Adaptive entropy - prevents premature convergence, maintains exploration
adaptive_entropy = true
adaptive_entropy_start = 0.7   # Start at 70% of max entropy
adaptive_entropy_final = 0.2   # End at 20%
adaptive_entropy_warmup = 0.1  # 10% warmup period
value_coef = 0.5         # Value loss weight
max_grad_norm = 0.5      # Gradient clipping
target_kl = 0.02         # KL early stopping
normalize_obs = false    # Board state doesn't need normalization

# Training - board games need more samples
total_timesteps = 20_000_000
num_epochs = 4
num_minibatches = 4
adam_epsilon = 1e-5

# Network - MLP (default)
# 3 layers of 128 hidden units (~45k MACs)
hidden_size = 128
num_hidden = 3
activation = "relu"

# CNN Alternative - E3a architecture (~45k MACs, matches MLP compute)
# 3x3 same padding, 3 layers: 8→6→8 filters, 7×7 receptive field (full board width)
# To use CNN instead of MLP, uncomment network_type below:
# network_type = "cnn"
num_conv_layers = 3
conv_channels = [8, 6, 8]
kernel_size = 3
cnn_fc_hidden_size = 64    # FC layer after conv: 336 (8×6×7) → 64 → heads
cnn_num_fc_layers = 1

# Checkpointing
run_dir = "runs"
checkpoint_freq = 50_000

# Logging
log_freq = 5_000

# Experiment
seed = 42
