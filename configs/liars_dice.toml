# PPO configuration for Liar's Dice
# 4-player bluffing game with hidden information

# Environment
env = "liars_dice"
num_envs = 32
num_steps = 64          # Longer rollouts for complex game dynamics
reward_shaping_coef = 0.05  # Per-round survival bonus (0.0 for pure zero-sum)

# PPO hyperparameters
learning_rate = 0.0003
lr_anneal = false
gamma = 0.99
gae_lambda = 0.95
clip_epsilon = 0.1
clip_value = false
entropy_coef = 0.01      # Higher entropy for bluffing exploration
entropy_anneal = false
value_coef = 0.5
max_grad_norm = 0.5
target_kl = 0.02
normalize_obs = false   # Discrete game state doesn't need normalization

# Training - 4-player games need more samples
total_timesteps = 30_000_000
num_epochs = 4
num_minibatches = 4
adam_epsilon = 1e-5

# Network - larger for imperfect information game
hidden_size = 256
num_hidden = 4
activation = "relu"

# Checkpointing
run_dir = "runs"
checkpoint_freq = 100_000

# Logging
log_freq = 10_000

# Experiment
seed = 42
