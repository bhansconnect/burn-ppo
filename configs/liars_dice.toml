# PPO configuration for Liar's Dice
# 4-player bluffing game with hidden information

# Environment
env = "liars_dice"
num_envs = 128          # More envs improves throughput with opponent pool
num_steps = 64          # Longer rollouts for complex game dynamics
reward_shaping_coef = 0.05  # Per-round survival bonus (0.0 for pure zero-sum)

# Opponent pool: train against historical versions (0.0 = disabled)
# 25% of envs play against historical checkpoints, 75% self-play
opponent_pool_fraction = 0.25
opponent_select_alpha = 0.1      # EMA smoothing for win rate tracking (batch per rotation)
opponent_select_exponent = 1.0   # P(select) ‚àù (1-win_rate)^p, higher = focus on hard opponents

# PPO hyperparameters
learning_rate = 0.001
gamma = 0.99
gae_lambda = 0.95
clip_epsilon = 0.1
clip_value = false
entropy_coef = 0.05      # Higher entropy for bluffing exploration
value_coef = 1.0
max_grad_norm = 0.5
target_kl = 0.02
normalize_obs = false   # Discrete game state doesn't need normalization

# Training - 4-player games need more samples
total_steps = 30_000_000
num_epochs = 6
num_minibatches = 4
adam_epsilon = 1e-5

# Network - larger for imperfect information game
hidden_size = 512
num_hidden = 3
activation = "relu"

# Checkpointing
run_dir = "runs"
checkpoint_freq = 100_000

# Logging
log_freq = 10_000

# Experiment
seed = 42
