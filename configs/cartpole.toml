# PPO configuration for CartPole
# Optimized based on ablation experiments (see docs/DESIGN.md)
#
# SCALING NOTES:
# PPO learns via rollout count, not just sample count. Key relationships:
#   rollouts = total_timesteps / (num_envs * num_steps)
#   batch_size = num_envs * num_steps
#   minibatch_size = batch_size / num_minibatches
#
# When increasing num_envs for better GPU utilization:
#   1. Scale total_timesteps proportionally to maintain rollout count
#   2. Scale num_minibatches to keep minibatch_size ~128-256
#   3. Keep num_epochs * num_minibatches similar for same updates/rollout
#
# Example scaling from 4 to 16 envs:
#   num_envs = 16, total_timesteps = 800_000 (4x samples for same rollouts)
#   num_minibatches = 8 (keeps minibatch_size = 256)
#   num_epochs = 2 (keeps gradient updates per rollout similar)

# Environment
env = "cartpole"
num_envs = 4         # 4 envs optimal for 200k steps. Scale with total_timesteps.
num_steps = 128      # Steps per rollout. 64-256 typical.

# PPO hyperparameters
learning_rate = 0.001    # 4x higher than ICLR default (2.5e-4) - critical for fast learning
                         # Range: 1e-4 to 3e-3
lr_anneal = false        # Linear decay to 0. Enable for longer runs (>500k steps)
gamma = 0.99             # Discount factor. 0.99 standard. Lower (0.95-0.98) for shorter horizons.
gae_lambda = 0.95        # GAE smoothing. 0.95 standard. Lower = more bias, less variance.
clip_epsilon = 0.2       # PPO clip range. 0.1-0.3 typical. Lower = more conservative updates.
clip_value = true        # Value function clipping. May help or hurt.
entropy_coef = 0.01      # Entropy bonus for exploration. Range: 0.001 to 0.1.
entropy_anneal = false   # Decay entropy coef over training. Enable with higher initial (0.05).
value_coef = 0.5         # Value loss weight. 0.5 standard.
max_grad_norm = 0.5      # Gradient clipping. Prevents exploding gradients.
# target_kl = 0.02      # KL early stopping. Optional for CartPole.
normalize_obs = false    # CartPole observations are already well-scaled.
                         # Normalization can hurt simple environments.

# Training
total_timesteps = 200_000    # Sufficient for CartPole. Connect Four needs 2-5M.
num_epochs = 4               # Update epochs per rollout. 3-10 typical.
num_minibatches = 4          # Minibatches per epoch. batch_size = num_envs * num_steps / num_minibatches
adam_epsilon = 1e-5          # NOT default 1e-8! ICLR recommends 1e-5 for PPO stability.

# Network
hidden_size = 64         # Neurons per hidden layer. CartPole: 64, Connect Four: 128-256.
num_hidden = 2           # Hidden layers. 2-4 typical. Deeper needs more data.
activation = "relu"      # "relu" or "tanh". ReLU significantly outperforms tanh for CartPole.

# Checkpointing
run_dir = "runs"
checkpoint_freq = 10_000

# Logging
log_freq = 1_000

# Experiment
seed = 42
# run_name = "my_experiment"  # Optional, auto-generated if omitted
