# PPO configuration for CartPole
# Optimized based on ablation experiments
#
# Key findings:
# - Observation includes normalized step (0-1) for stable value learning
# - High learning rate (0.001) works well with step in observation
# - No LR annealing needed for runs up to 500k steps
# - Value clipping disabled (research shows it often hurts)

# Environment
env = "cartpole"
num_envs = 32
num_steps = 128

# PPO hyperparameters
learning_rate = 0.001    # Works well with step in observation
gamma = 0.99
gae_lambda = 0.95
clip_epsilon = 0.2
clip_value = false       # Disabled - allows faster value function adaptation
entropy_coef = 0.01
value_coef = 0.5
max_grad_norm = 0.5
normalize_obs = true

# Training
total_steps = 1_000_000
num_epochs = 4
num_minibatches = 4
adam_epsilon = 1e-5

# Network
hidden_size = 64
num_hidden = 2
activation = "relu"

# Checkpointing
run_dir = "runs"
checkpoint_freq = 10_000
log_freq = 1_000
seed = 42
