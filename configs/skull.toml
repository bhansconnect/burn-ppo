# PPO configuration for Skull

env = "skull"
num_envs = 128
num_steps = 128

# Enable reward shaping for intermediate learning signals
reward_shaping_coef = 0.0
learning_rate = [[0.001, 0], [0.0003, 20_000_000], [0.0001, 80_000_000], [0.0, 100_000_000]]

# GAE parameters for multi-player imperfect info game
gamma = 0.95
gae_lambda = 0.92

# PPO hyperparameters
clip_epsilon = 0.10    # Lower threshold to trigger more clipping
entropy_coef = 0.03    # High exploration for bluffing
value_coef = 0.5       # Strong value function learning
max_grad_norm = 0.5
target_kl = 0.02       # Early stopping if KL exceeds this

# Training duration
total_steps = 100_000_000

# More update iterations per rollout
num_epochs = 4
num_minibatches = 8

# Larger network for bluffing complexity
hidden_size = 256
num_hidden = 3
activation = "relu"

# Self-play settings
opponent_pool_fraction = 0.3
opponent_select_exponent = 2.0

checkpoint_freq = 100_000
log_freq = 10_000

# Player count configuration (2-6 supported) - MUST BE AT END
# Options:
#   type = "Fixed", count = 4
#   type = "UniformRandom", min = 3, max = 6
#   type = "WeightedRandom", weights = [0.05, 0.2, 0.3, 0.3, 0.15]  # for 2-6 players
#   type = "Curriculum", min = 3, max = 6, warmup_steps = 5000000
[player_count]
type = "Fixed"
count = 4
