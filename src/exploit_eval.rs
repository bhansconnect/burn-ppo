//! Best-response exploitability evaluation
//!
//! Trains small counter-policies against saved checkpoints to measure exploitability.
//! Exploitability = expected reward of best-response policy vs victim policy.

use anyhow::{Context, Result};
use burn::module::AutodiffModule;
use burn::optim::AdamConfig;
use burn::prelude::*;
use burn::tensor::backend::AutodiffBackend;
use plotters::prelude::*;
use rand::SeedableRng;
use serde::{Deserialize, Serialize};
use std::collections::HashSet;
use std::path::{Path, PathBuf};
use std::time::Instant;

use crate::checkpoint::{load_metadata, CheckpointManager};
use crate::config::{Config, ExploitEvalArgs};
use crate::dispatch_env;
use crate::env::Environment;
use crate::network::ActorCritic;
use crate::ppo::{compute_gae, compute_gae_multiplayer, ppo_update, RolloutBuffer};

// ===== Helper Functions =====

/// Check if a path is a checkpoint directory (contains metadata.json)
fn is_checkpoint_dir(path: &Path) -> bool {
    path.is_dir() && path.join("metadata.json").exists()
}

// ===== Data Structures =====
// Note: These structures will be used in future phases (2-5)

#[derive(Clone, Debug)]
pub struct ExploitEvalConfig {
    pub br_min_steps: usize,
    pub br_max_steps: usize,
    pub br_plateau_window: usize, // 0 = disable early stopping
    pub br_plateau_threshold: f64,
    pub br_lr: f64,
    pub br_envs: usize,
    pub br_hidden_size: usize,
    pub br_num_hidden: usize,
    pub br_activation: String,
    pub eval_games: usize,
    pub eval_temp: Option<f32>,
    pub seed: u64,
}

#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct ExploitabilityResult {
    pub checkpoint_name: String,
    pub checkpoint_step: usize,

    // Primary metric
    pub br_advantage: f32, // BR_avg_reward - victim_avg_reward

    // Secondary metrics
    pub br_win_rate: f32,       // Win rate of trained BR policy
    pub br_avg_return: f32,     // BR's average return
    pub victim_avg_return: f32, // Victim's average return
    pub br_avg_margin: f32,     // Average score margin when BR wins

    // Meta
    pub br_training_steps: usize, // Actual steps trained (may be less than max due to early stopping)
    pub br_training_time_secs: f32, // Time to train BR policy
    pub eval_games_played: usize, // Number of evaluation games
    pub br_still_learning: bool,  // true if stopped at max_steps while still improving
}

#[derive(Serialize, Deserialize)]
pub struct ExploitEvalResults {
    pub config: ExploitEvalConfigSummary,
    pub environment: String,
    pub results: Vec<ExploitabilityResult>,
    pub timestamp: String,
}

#[derive(Serialize, Deserialize)]
pub struct ExploitEvalConfigSummary {
    pub br_min_steps: usize,
    pub br_max_steps: usize,
    pub br_plateau_window: usize,
    pub br_lr: f64,
    pub br_hidden_size: usize,
    pub br_num_hidden: usize,
    pub eval_games: usize,
}

impl ExploitEvalConfig {
    pub fn from_args(args: &ExploitEvalArgs) -> Self {
        Self {
            br_min_steps: args.br_min_steps,
            br_max_steps: args.br_max_steps,
            br_plateau_window: args.br_plateau_window,
            br_plateau_threshold: args.br_plateau_threshold,
            br_lr: args.br_lr,
            br_envs: args.br_envs,
            br_hidden_size: args.br_hidden_size,
            br_num_hidden: args.br_num_hidden,
            br_activation: args.br_activation.clone(),
            eval_games: args.eval_games,
            eval_temp: args.eval_temp,
            seed: args.seed.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .expect("Time went backwards")
                    .as_secs()
            }),
        }
    }
}

// ===== Checkpoint Enumeration =====
// Reused from tournament.rs

fn enumerate_checkpoints(checkpoints_dir: &Path) -> Result<Vec<PathBuf>> {
    let mut checkpoints: Vec<PathBuf> = std::fs::read_dir(checkpoints_dir)
        .context("Failed to read checkpoints directory")?
        .filter_map(Result::ok)
        .filter(|e| {
            e.file_name()
                .to_str()
                .is_some_and(|n| n.starts_with("step_"))
        })
        .map(|e| e.path())
        .collect();

    // Sort by step number
    checkpoints.sort_by(|a, b| {
        let step_a = a
            .file_name()
            .and_then(|n| n.to_str())
            .and_then(|n| n.strip_prefix("step_"))
            .and_then(|s| s.parse::<usize>().ok())
            .unwrap_or(0);
        let step_b = b
            .file_name()
            .and_then(|n| n.to_str())
            .and_then(|n| n.strip_prefix("step_"))
            .and_then(|s| s.parse::<usize>().ok())
            .unwrap_or(0);
        step_a.cmp(&step_b)
    });

    Ok(checkpoints)
}

/// Select N evenly distributed interior checkpoints
fn select_evenly_spaced(checkpoints: &[PathBuf], n: usize) -> Vec<PathBuf> {
    if n >= checkpoints.len() {
        return checkpoints.to_vec();
    }
    if n == 0 {
        return Vec::new();
    }

    let len = checkpoints.len();
    let mut selected = Vec::with_capacity(n);

    for k in 1..=n {
        // Position at k/(n+1) of the array
        let idx = ((len * k) / (n + 1)).min(len - 1);
        selected.push(checkpoints[idx].clone());
    }

    selected
}

/// Get the "best" checkpoint from a checkpoints directory
fn get_best_checkpoint(checkpoints_dir: &Path) -> Option<PathBuf> {
    let best_symlink = checkpoints_dir.join("best");

    // If "best" symlink exists, use it
    if best_symlink.exists() {
        let resolved = if best_symlink.is_symlink() {
            best_symlink.read_link().ok().map(|target| {
                if target.is_absolute() {
                    target
                } else {
                    checkpoints_dir.join(target)
                }
            })
        } else {
            Some(best_symlink)
        };

        if let Some(path) = resolved {
            if is_checkpoint_dir(&path) {
                return Some(path);
            }
        }
    }

    // Fallback: use latest checkpoint
    let checkpoints = enumerate_checkpoints(checkpoints_dir).ok()?;
    if checkpoints.is_empty() {
        return None;
    }

    eprintln!(
        "Warning: No 'best' symlink in {}, falling back to latest checkpoint",
        checkpoints_dir.display()
    );
    checkpoints.last().cloned()
}

/// Select checkpoints with priority: best, latest, then evenly distributed
fn select_checkpoints_with_priority(
    checkpoints_dir: &Path,
    checkpoints: &[PathBuf],
    limit: usize,
) -> Vec<PathBuf> {
    if limit == 0 || checkpoints.is_empty() {
        return Vec::new();
    }

    let best = get_best_checkpoint(checkpoints_dir);
    let latest = checkpoints.last().cloned();

    if limit == 1 {
        // Just best (or latest as final fallback)
        return best.or(latest).into_iter().collect();
    }

    // Best + latest (deduplicated) + evenly distributed from remaining
    let mut result = Vec::new();
    let mut excluded: HashSet<PathBuf> = HashSet::new();

    if let Some(b) = &best {
        result.push(b.clone());
        excluded.insert(b.clone());
    }
    if let Some(l) = &latest {
        if !excluded.contains(l) {
            result.push(l.clone());
            excluded.insert(l.clone());
        }
    }

    // Get remaining checkpoints (excluding best and latest)
    let remaining: Vec<PathBuf> = checkpoints
        .iter()
        .filter(|c| !excluded.contains(*c))
        .cloned()
        .collect();

    // Select (limit - result.len()) evenly from remaining
    let extra_needed = limit.saturating_sub(result.len());
    let extra = select_evenly_spaced(&remaining, extra_needed);
    result.extend(extra);

    result
}

/// Enumerate and select checkpoints based on path and selection strategy
pub fn enumerate_and_select_checkpoints(
    path: &Path,
    limit: Option<usize>,
    selection: &str,
) -> Result<Vec<PathBuf>> {
    // Determine checkpoints directory
    let checkpoints_dir = if path.join("checkpoints").is_dir() {
        // Run folder
        path.join("checkpoints")
    } else {
        // Assume path IS the checkpoints directory
        path.to_path_buf()
    };

    if !checkpoints_dir.is_dir() {
        anyhow::bail!(
            "Checkpoints directory not found: {}",
            checkpoints_dir.display()
        );
    }

    let all_checkpoints = enumerate_checkpoints(&checkpoints_dir)?;
    if all_checkpoints.is_empty() {
        anyhow::bail!("No checkpoints found in {}", checkpoints_dir.display());
    }

    // Apply selection strategy
    let selected = match selection {
        "all" => all_checkpoints,
        "best" => {
            let best = get_best_checkpoint(&checkpoints_dir).with_context(|| {
                format!("No 'best' checkpoint in {}", checkpoints_dir.display())
            })?;
            vec![best]
        }
        "latest" => vec![all_checkpoints
            .last()
            .cloned()
            .expect("Checked empty above")],
        "evenly-spaced" => {
            if let Some(lim) = limit {
                select_checkpoints_with_priority(&checkpoints_dir, &all_checkpoints, lim)
            } else {
                all_checkpoints
            }
        }
        other => anyhow::bail!("Unknown selection strategy: '{other}'"),
    };

    Ok(selected)
}

// ===== BR Training Functions =====

/// Return type for BR policy training: (model, steps, `time_secs`, `avg_return`, `still_learning`)
type BrTrainingResult<B> = (ActorCritic<B>, usize, f32, f32, bool);

/// Simple episode statistics
#[derive(Debug, Clone)]
struct SimpleEpisodeStats {
    total_reward: f32,
    _length: usize,
}

impl SimpleEpisodeStats {
    fn total_reward(&self) -> f32 {
        self.total_reward
    }
}

/// Collect rollouts where BR policy plays against a fixed frozen opponent
///
/// This is a simplified training collection that plays games where BR takes one position
/// and the victim takes all other positions. Only BR's experiences are collected for training.
///
/// Returns `(episode_stats, num_steps_collected)`
#[expect(
    clippy::too_many_arguments,
    clippy::cast_possible_wrap,
    clippy::cast_sign_loss,
    clippy::too_many_lines,
    reason = "Rollout collection requires many parameters; casts are for small positive values"
)]
fn collect_rollouts_vs_fixed_opponent<B, E>(
    br_model: &ActorCritic<B>,
    victim_model: &ActorCritic<B>,
    _num_envs: usize,
    target_steps: usize,
    buffer: &mut RolloutBuffer<B>,
    seed: u64,
    device: &B::Device,
    rng: &mut impl rand::Rng,
    num_players: usize,
) -> (Vec<SimpleEpisodeStats>, usize)
where
    B: Backend,
    B::FloatElem: Into<f32>,
    E: Environment,
{
    let obs_dim = E::OBSERVATION_DIM;
    let max_players = E::NUM_PLAYERS; // Max players for buffer sizing

    // Track episode completions
    let mut completed_episodes = Vec::new();

    // Buffers for batch collection (collect BR experiences only)
    let mut all_obs: Vec<f32> = Vec::with_capacity(target_steps * obs_dim);
    let mut all_actions: Vec<i64> = Vec::with_capacity(target_steps);
    let mut all_rewards: Vec<f32> = Vec::with_capacity(target_steps);
    let mut all_dones: Vec<f32> = Vec::with_capacity(target_steps);
    let mut all_log_probs: Vec<f32> = Vec::with_capacity(target_steps);

    // Multi-player data
    let mut all_values_flat: Vec<f32> = Vec::with_capacity(target_steps * max_players);
    let mut all_rewards_flat: Vec<f32> = Vec::with_capacity(target_steps * max_players);
    let mut all_acting_players: Vec<i64> = Vec::with_capacity(target_steps);
    let mut br_positions_used: Vec<usize> = Vec::with_capacity(target_steps / 10); // Track BR positions for later

    let mut steps_collected = 0;
    let mut env_idx = 0usize;

    // Play games until we collect enough BR steps
    while steps_collected < target_steps {
        let mut env = E::new(seed + env_idx as u64);
        env.set_num_players(num_players);
        let actual_player_count = num_players;
        let br_position = env_idx % actual_player_count; // Cycle BR through positions for fairness
        env_idx += 1;

        let mut episode_reward = 0.0f32;
        let mut episode_length = 0usize;
        let mut obs = env.reset();
        let mut is_done = false;
        br_positions_used.push(br_position); // Track this episode's BR position

        while !is_done && steps_collected < target_steps {
            let current_player = env.current_player();
            let is_br_turn = current_player == br_position;

            // Create observation tensor
            let obs_tensor: Tensor<B, 2> =
                Tensor::<B, 1>::from_floats(obs.as_slice(), device).reshape([1, obs_dim]);

            // Choose model based on whose turn it is
            let model = if is_br_turn { br_model } else { victim_model };

            // Handle CTDE networks: separate forward passes for actor and critic
            let (logits, values) = if model.is_ctde() {
                let global_state = env.global_state();
                let global_state_dim = global_state.len();
                let global_state_tensor =
                    Tensor::<B, 1>::from_floats(global_state.as_slice(), device)
                        .reshape([1, global_state_dim]);
                let logits = model.forward_actor(obs_tensor);
                let values = model.forward_critic(global_state_tensor);
                (logits, values)
            } else {
                model.forward(obs_tensor)
            };

            // Sample action with action masking
            let probs = burn::tensor::activation::softmax(logits, 1);
            let mut probs_data: Vec<f32> = probs.clone().into_data().to_vec().expect("probs");

            // Apply action mask
            if let Some(mask) = env.action_mask() {
                for (i, &valid) in mask.iter().enumerate() {
                    if !valid {
                        probs_data[i] = 0.0;
                    }
                }
                // Renormalize
                let sum: f32 = probs_data.iter().sum();
                if sum > 0.0 {
                    for p in &mut probs_data {
                        *p /= sum;
                    }
                }
            }

            let action = sample_action(&probs_data, rng);

            // Compute log prob for BR turns
            let log_prob: f32 = if is_br_turn {
                let action_tensor: Tensor<B, 1, Int> =
                    Tensor::<B, 1, Int>::from_ints([action as i64], device);
                let action_log_probs = probs.log().gather(1, action_tensor.unsqueeze());
                action_log_probs
                    .into_data()
                    .to_vec::<f32>()
                    .expect("log_prob")[0]
            } else {
                0.0 // Not used for victim turns
            };

            // Step environment
            let (next_obs, rewards, step_done) = env.step(action);
            is_done = step_done;

            // Extract BR's reward
            let br_reward = if br_position < rewards.len() {
                rewards[br_position]
            } else {
                0.0
            };

            episode_length += 1;

            // Only store BR's experiences (on-policy requirement)
            if is_br_turn {
                all_obs.extend_from_slice(&obs);
                all_actions.push(action as i64);
                all_rewards.push(br_reward);
                all_dones.push(if is_done { 1.0 } else { 0.0 });

                let value_data: Vec<f32> = values.into_data().to_vec().expect("values");
                all_log_probs.push(log_prob);

                // Store multi-player data (pad to max_players for buffer compatibility)
                let mut padded_values = value_data;
                padded_values.resize(max_players, 0.0);
                all_values_flat.extend(&padded_values);
                let mut padded_rewards = rewards.clone();
                padded_rewards.resize(max_players, 0.0);
                all_rewards_flat.extend(&padded_rewards);
                all_acting_players.push(current_player as i64);

                episode_reward += br_reward;
                steps_collected += 1;
            }

            obs = next_obs;
        }

        // Record episode completion
        completed_episodes.push(SimpleEpisodeStats {
            total_reward: episode_reward,
            _length: episode_length,
        });
    }

    // Transfer data to buffer
    if !all_obs.is_empty() {
        let num_envs_used = 1; // We'll pretend it's one env with variable steps
        let actual_steps = steps_collected;

        // Reshape data to match buffer expectations: [num_steps, num_envs, ...]
        // For simplicity, we'll use num_envs=1 and put all steps in sequence
        buffer.observations = Tensor::<B, 1>::from_floats(all_obs.as_slice(), device).reshape([
            actual_steps,
            num_envs_used,
            obs_dim,
        ]);
        buffer.actions = Tensor::<B, 1, Int>::from_ints(all_actions.as_slice(), device)
            .reshape([actual_steps, num_envs_used]);
        buffer.rewards = Tensor::<B, 1>::from_floats(all_rewards.as_slice(), device)
            .reshape([actual_steps, num_envs_used]);
        buffer.dones = Tensor::<B, 1>::from_floats(all_dones.as_slice(), device)
            .reshape([actual_steps, num_envs_used]);
        buffer.log_probs = Tensor::<B, 1>::from_floats(all_log_probs.as_slice(), device)
            .reshape([actual_steps, num_envs_used]);

        if max_players > 1 {
            buffer.all_values = Tensor::<B, 1>::from_floats(all_values_flat.as_slice(), device)
                .reshape([actual_steps, num_envs_used, max_players]);
            buffer.all_rewards = Tensor::<B, 1>::from_floats(all_rewards_flat.as_slice(), device)
                .reshape([actual_steps, num_envs_used, max_players]);
            buffer.acting_players =
                Tensor::<B, 1, Int>::from_ints(all_acting_players.as_slice(), device)
                    .reshape([actual_steps, num_envs_used]);

            // For single-player view, extract acting player's value (which should be BR's value)
            // Since we only stored BR's turns, all acting_players should be BR's position
            let br_values: Vec<f32> = all_values_flat
                .chunks(max_players)
                .zip(all_acting_players.iter())
                .map(|(chunk, &acting_player)| {
                    let idx = acting_player as usize;
                    chunk.get(idx).copied().unwrap_or(0.0)
                })
                .collect();
            buffer.values = Tensor::<B, 1>::from_floats(br_values.as_slice(), device)
                .reshape([actual_steps, num_envs_used]);
        } else {
            buffer.values = Tensor::<B, 1>::from_floats(all_values_flat.as_slice(), device)
                .reshape([actual_steps, num_envs_used]);
        }

        // Note: RolloutBuffer doesn't have size/num_envs fields - those are tracked implicitly
    }

    (completed_episodes, steps_collected)
}

/// Train a best-response policy from scratch against a frozen victim checkpoint
///
/// Trains BR policy by playing against the frozen victim, collecting only BR's experiences.
/// Returns `(trained_model, actual_steps, training_time_secs, avg_episode_return)`
#[expect(clippy::too_many_lines, reason = "Complex training loop needed")]
fn train_br_policy<B, E>(
    victim_checkpoint: &Path,
    config: &ExploitEvalConfig,
    num_players: usize,
    device: &B::Device,
) -> Result<BrTrainingResult<B::InnerBackend>>
where
    B: AutodiffBackend,
    <B::InnerBackend as Backend>::FloatElem: Into<f32>,
    E: Environment,
{
    let train_start = Instant::now();

    // Load frozen victim model
    let victim_config = Config::default();
    let (victim_model, _victim_meta) =
        CheckpointManager::load::<B::InnerBackend>(victim_checkpoint, &victim_config, device)?;

    // Create small BR model with custom config
    let br_config = Config {
        hidden_size: config.br_hidden_size,
        num_hidden: config.br_num_hidden,
        activation: config.br_activation.clone(),
        network_type: "mlp".to_string(),
        learning_rate: crate::schedule::Schedule::constant(config.br_lr),
        entropy_coef: crate::schedule::Schedule::constant(0.01),
        num_epochs: 4,
        num_minibatches: 4,
        ..Config::default()
    };

    let mut br_model: ActorCritic<B> = ActorCritic::new(
        E::OBSERVATION_DIM,
        E::OBSERVATION_SHAPE,
        E::ACTION_COUNT,
        num_players,
        None, // BR model uses MLP, not CTDE
        &br_config,
        device,
    );

    // Initialize optimizer
    let optim_config = AdamConfig::new().with_epsilon(1e-5);
    let mut optimizer = optim_config.init();

    // Create training environments
    let mut rng = rand::rngs::StdRng::seed_from_u64(config.seed);
    let num_steps_per_rollout = 128;

    // Initialize rollout buffer (stores only BR's experiences)
    let mut buffer: RolloutBuffer<B::InnerBackend> = RolloutBuffer::new(
        num_steps_per_rollout,
        config.br_envs,
        E::OBSERVATION_DIM,
        None,
        E::NUM_PLAYERS as u8,
        device,
    );

    // Training loop with early stopping
    let mut step = 0;
    let mut recent_returns: Vec<f32> = Vec::new();
    let rolling_window_size = 500;
    let mut best_rolling_avg = f32::NEG_INFINITY;
    let mut steps_without_improvement = 0;
    let mut stopped_due_to_plateau = false;

    while step < config.br_max_steps {
        // Collect rollouts where BR plays against frozen victim
        let inference_model = br_model.valid();
        let (episodes, num_collected) = collect_rollouts_vs_fixed_opponent::<B::InnerBackend, E>(
            &inference_model,
            &victim_model,
            config.br_envs,
            num_steps_per_rollout,
            &mut buffer,
            config.seed + step as u64,
            device,
            &mut rng,
            num_players,
        );

        // Track episode returns for early stopping
        for ep in &episodes {
            recent_returns.push(ep.total_reward());
        }

        // If we didn't collect enough steps, break early
        if num_collected < num_steps_per_rollout / 2 {
            break;
        }

        // Compute GAE
        if E::NUM_PLAYERS == 1 {
            compute_gae(
                &mut buffer,
                Tensor::<B::InnerBackend, 1>::zeros([config.br_envs], device),
                br_config.gamma as f32,
                br_config.gae_lambda as f32,
                device,
            );
        } else {
            compute_gae_multiplayer(
                &mut buffer,
                Tensor::<B::InnerBackend, 2>::zeros([config.br_envs, E::NUM_PLAYERS], device),
                br_config.gamma as f32,
                br_config.gae_lambda as f32,
                E::NUM_PLAYERS as u8,
                device,
            );
        }

        // PPO update
        let (updated_model, _metrics) = ppo_update(
            br_model,
            &buffer,
            &mut optimizer,
            &br_config,
            config.br_lr,
            0.01, // entropy_coef
            E::ACTION_COUNT,
            &mut rng,
            None,
        );
        br_model = updated_model;

        step += num_steps_per_rollout;

        // Early stopping check
        if step >= config.br_min_steps && config.br_plateau_window > 0 {
            let window_start = recent_returns.len().saturating_sub(rolling_window_size);
            if recent_returns.len() > window_start {
                let rolling_avg: f32 = recent_returns[window_start..].iter().sum::<f32>()
                    / (recent_returns.len() - window_start) as f32;

                if rolling_avg > best_rolling_avg + config.br_plateau_threshold as f32 {
                    best_rolling_avg = rolling_avg;
                    steps_without_improvement = 0;
                } else {
                    steps_without_improvement += num_steps_per_rollout;
                }

                if steps_without_improvement >= config.br_plateau_window {
                    stopped_due_to_plateau = true;
                    break;
                }
            }
        }
    }

    let train_time = train_start.elapsed().as_secs_f32();
    let avg_return = if recent_returns.is_empty() {
        0.0
    } else {
        recent_returns.iter().sum::<f32>() / recent_returns.len() as f32
    };

    // Detect if we hit max_steps while still learning
    let hit_max_steps = step >= config.br_max_steps;
    let still_learning = !stopped_due_to_plateau && hit_max_steps;

    Ok((
        br_model.valid(),
        step,
        train_time,
        avg_return,
        still_learning,
    ))
}

/// Evaluate BR policy against victim by playing games
///
/// Returns exploitability and detailed statistics
fn evaluate_br_vs_victim<B, E>(
    br_model: &ActorCritic<B>,
    victim_checkpoint: &Path,
    config: &ExploitEvalConfig,
    num_players: usize,
    device: &B::Device,
) -> Result<ExploitabilityResult>
where
    B: Backend,
    B::FloatElem: Into<f32>,
    E: Environment,
{
    // Load victim model
    let victim_config = Config::default();
    let (victim_model, victim_meta) =
        CheckpointManager::load::<B>(victim_checkpoint, &victim_config, device)?;

    // Create evaluation environments
    let mut rng = rand::rngs::StdRng::seed_from_u64(config.seed + 999);
    let num_games = config.eval_games;

    // Play games with position permutation
    let mut br_total_reward = 0.0f32;
    let mut victim_total_reward = 0.0f32;
    let mut br_wins = 0usize;
    let mut games_played = 0usize;

    for game_idx in 0..num_games {
        let mut env = E::new(config.seed + game_idx as u64);
        env.set_num_players(num_players);
        let actual_player_count = num_players;

        // Determine which model plays which position (cycling for fairness)
        let br_position = game_idx % actual_player_count;

        // Reset environment and get initial observation
        let mut obs = env.reset();
        let mut done = false;

        while !done {
            let current_player = env.current_player();

            // Create observation tensor
            let obs_tensor: Tensor<B, 2> = Tensor::<B, 1>::from_floats(obs.as_slice(), device)
                .reshape([1, E::OBSERVATION_DIM]);

            // Choose model based on position
            let model = if current_player == br_position {
                br_model
            } else {
                &victim_model
            };

            // Handle CTDE networks: separate forward passes for actor and critic
            let (logits, _values) = if model.is_ctde() {
                let global_state = env.global_state();
                let global_state_dim = global_state.len();
                let global_state_tensor =
                    Tensor::<B, 1>::from_floats(global_state.as_slice(), device)
                        .reshape([1, global_state_dim]);
                let logits = model.forward_actor(obs_tensor);
                let values = model.forward_critic(global_state_tensor);
                (logits, values)
            } else {
                model.forward(obs_tensor)
            };

            // Sample action (with temperature if specified)
            let probs = if let Some(temp) = config.eval_temp {
                burn::tensor::activation::softmax(logits / temp, 1)
            } else {
                burn::tensor::activation::softmax(logits, 1)
            };

            let mut probs_data: Vec<f32> = probs.into_data().to_vec().expect("probs");

            // Apply action mask
            if let Some(mask) = env.action_mask() {
                for (i, &valid) in mask.iter().enumerate() {
                    if !valid {
                        probs_data[i] = 0.0;
                    }
                }
                // Renormalize
                let sum: f32 = probs_data.iter().sum();
                if sum > 0.0 {
                    for p in &mut probs_data {
                        *p /= sum;
                    }
                }
            }

            let action = sample_action(&probs_data, &mut rng);

            // Step environment
            let (next_obs, rewards, is_done) = env.step(action);
            obs = next_obs;
            done = is_done;

            if done {
                // Record results (with bounds checking)
                if br_position < rewards.len() {
                    br_total_reward += rewards[br_position];

                    // Find victim position(s) and average their rewards
                    let victim_reward: f32 = rewards
                        .iter()
                        .enumerate()
                        .filter(|(i, _)| *i != br_position)
                        .map(|(_, r)| r)
                        .sum::<f32>()
                        / (actual_player_count - 1).max(1) as f32;
                    victim_total_reward += victim_reward;

                    // Check if BR won (highest reward)
                    if rewards[br_position]
                        >= rewards.iter().copied().fold(f32::NEG_INFINITY, f32::max)
                    {
                        br_wins += 1;
                    }

                    games_played += 1;
                } else {
                    // If br_position is out of bounds, skip this game
                    eprintln!(
                        "Warning: br_position {} >= rewards.len() {}, skipping game {}",
                        br_position,
                        rewards.len(),
                        game_idx
                    );
                }
            }
        }
    }

    let br_advantage = (br_total_reward - victim_total_reward) / games_played as f32;
    let br_win_rate = br_wins as f32 / games_played as f32;
    let br_avg_return = br_total_reward / games_played as f32;
    let victim_avg_return = victim_total_reward / games_played as f32;

    Ok(ExploitabilityResult {
        checkpoint_name: victim_checkpoint
            .file_name()
            .context("Failed to extract filename from checkpoint path")?
            .to_string_lossy()
            .to_string(),
        checkpoint_step: victim_meta.step,
        br_advantage,
        br_win_rate,
        br_avg_return,
        victim_avg_return,
        br_avg_margin: if br_wins > 0 {
            br_avg_return - victim_avg_return
        } else {
            0.0
        },
        br_training_steps: 0,       // Will be filled in by caller
        br_training_time_secs: 0.0, // Will be filled in by caller
        eval_games_played: games_played,
        br_still_learning: false, // Will be filled in by caller
    })
}

/// Sample action from probability distribution
fn sample_action(probs: &[f32], rng: &mut impl rand::Rng) -> usize {
    let roll: f32 = rng.gen();
    let mut cumsum = 0.0;
    for (i, &p) in probs.iter().enumerate() {
        cumsum += p;
        if roll < cumsum {
            return i;
        }
    }
    probs.len() - 1
}

// ===== Output Functions =====

/// Save results to JSON file
fn save_results_json(
    results: &[ExploitabilityResult],
    config: &ExploitEvalConfig,
    env_name: &str,
    output_path: &Path,
) -> Result<()> {
    let timestamp = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .map_or_else(|_| "unknown".to_string(), |d| format!("{}", d.as_secs()));

    let full_results = ExploitEvalResults {
        config: ExploitEvalConfigSummary {
            br_min_steps: config.br_min_steps,
            br_max_steps: config.br_max_steps,
            br_plateau_window: config.br_plateau_window,
            br_lr: config.br_lr,
            br_hidden_size: config.br_hidden_size,
            br_num_hidden: config.br_num_hidden,
            eval_games: config.eval_games,
        },
        environment: env_name.to_string(),
        results: results.to_vec(),
        timestamp,
    };

    let json = serde_json::to_string_pretty(&full_results)?;
    std::fs::write(output_path, json)?;

    Ok(())
}

/// Generate exploitability graph
fn generate_graph(results: &[ExploitabilityResult], output_path: &Path) -> Result<()> {
    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();
    root.fill(&WHITE)?;

    // Draw exploitability chart
    draw_exploitability_subplot(&root, results)?;

    root.present()?;

    Ok(())
}

fn draw_exploitability_subplot(
    area: &DrawingArea<BitMapBackend, plotters::coord::Shift>,
    results: &[ExploitabilityResult],
) -> Result<()> {
    let data: Vec<(usize, f32)> = results
        .iter()
        .map(|r| (r.checkpoint_step, r.br_advantage))
        .collect();

    if data.is_empty() {
        return Ok(());
    }

    let max_step = data.iter().map(|(s, _)| *s).max().unwrap_or(1);
    let max_exploit = data
        .iter()
        .map(|(_, e)| *e)
        .fold(f32::NEG_INFINITY, f32::max);
    let min_exploit = data.iter().map(|(_, e)| *e).fold(f32::INFINITY, f32::min);

    let y_min = (min_exploit * 0.9).min(-0.1);
    let y_max = (max_exploit * 1.1).max(0.1);

    let mut chart = ChartBuilder::on(area)
        .caption("Exploitability Over Training", ("sans-serif", 35))
        .margin(15)
        .x_label_area_size(40)
        .y_label_area_size(60)
        .build_cartesian_2d(0usize..max_step, y_min..y_max)?;

    chart
        .configure_mesh()
        .x_desc("Training Step")
        .y_desc("Exploitability (BR advantage)")
        .draw()?;

    // Line series
    chart.draw_series(LineSeries::new(
        data.iter().map(|&(s, e)| (s, e)),
        &RED.mix(0.8),
    ))?;

    // Points
    chart.draw_series(
        data.iter()
            .map(|&(s, e)| Circle::new((s, e), 4, RED.filled())),
    )?;

    Ok(())
}

// ===== Main Entry Point =====

/// Run exploitability evaluation with backend dispatch
pub fn run_exploit_eval_with_backend<B: AutodiffBackend>(
    args: &ExploitEvalArgs,
    device: &B::Device,
) -> Result<()>
where
    <B::InnerBackend as Backend>::FloatElem: Into<f32>,
{
    // Enumerate checkpoints - either single checkpoint or run folder
    let checkpoints = if args.path.join("metadata.json").exists() {
        // Path is a single checkpoint directory
        vec![args.path.clone()]
    } else {
        // Path is a run folder - enumerate checkpoints
        enumerate_and_select_checkpoints(&args.path, args.limit, &args.selection)?
    };

    if checkpoints.is_empty() {
        anyhow::bail!("No checkpoints found in {}", args.path.display());
    }

    // Load first checkpoint to determine environment
    let first_meta = load_metadata(&checkpoints[0])?;
    let env_name = first_meta.env_name.clone();

    // Dispatch to environment-specific implementation
    dispatch_env!(
        &env_name,
        run_exploit_eval_env::<B, E>(args, &checkpoints, device)
    )
}

/// Run exploitability evaluation for a specific backend and environment
fn run_exploit_eval_env<B, E>(
    args: &ExploitEvalArgs,
    checkpoints: &[PathBuf],
    device: &B::Device,
) -> Result<()>
where
    B: AutodiffBackend,
    <B::InnerBackend as Backend>::FloatElem: Into<f32>,
    E: Environment,
{
    // Determine player count (same logic as tournaments)
    let num_players = if E::VARIABLE_PLAYER_COUNT {
        args.players.ok_or_else(|| {
            anyhow::anyhow!(
                "{} supports variable player counts. Use --players N to specify (e.g., --players 4)",
                E::NAME
            )
        })?
    } else {
        E::NUM_PLAYERS
    };

    println!("Exploit-Eval: Measuring exploitability via best-response training");
    println!("  Environment: {}", E::NAME);
    println!("  Players: {num_players}");
    println!("  Path: {}", args.path.display());
    println!("  Selection: {}", args.selection);
    println!(
        "  BR training: {}-{} steps @ lr={}",
        args.br_min_steps, args.br_max_steps, args.br_lr
    );
    println!("  Evaluation: {} games per checkpoint\n", args.eval_games);

    println!("Found {} checkpoint(s) to evaluate:", checkpoints.len());
    for (i, ckpt) in checkpoints.iter().enumerate() {
        let name = ckpt.file_name().and_then(|n| n.to_str()).unwrap_or("?");
        if let Ok(meta) = load_metadata(ckpt) {
            println!(
                "  {}: {} (step {}, {} players)",
                i + 1,
                name,
                meta.step,
                meta.num_players
            );
        } else {
            println!("  {}: {}", i + 1, name);
        }
    }

    println!();

    // Convert args to config
    let config = ExploitEvalConfig::from_args(args);

    // Evaluate each checkpoint
    let mut results = Vec::new();

    for (i, checkpoint_path) in checkpoints.iter().enumerate() {
        let ckpt_name = checkpoint_path
            .file_name()
            .context("Failed to extract filename from checkpoint path")?
            .to_string_lossy()
            .to_string();

        println!(
            "Evaluating checkpoint {}/{}: {}",
            i + 1,
            checkpoints.len(),
            ckpt_name
        );

        // Train BR policy
        println!("  Training BR policy...");
        let (br_model, br_steps, br_time, _avg_return, still_learning) =
            train_br_policy::<B, E>(checkpoint_path, &config, num_players, device)?;
        println!("  ✓ BR training complete: {br_steps} steps in {br_time:.1}s");

        // Warn if training stopped at max_steps while still improving
        if still_learning {
            println!(
                "  ⚠️  Warning: Training stopped at max_steps but policy may still be improving."
            );
            println!(
                "      Consider increasing --br-max-steps for better exploitability measurement."
            );
        }

        // Evaluate BR vs victim
        println!("  Evaluating exploitability...");
        let mut result = evaluate_br_vs_victim::<B::InnerBackend, E>(
            &br_model,
            checkpoint_path,
            &config,
            num_players,
            device,
        )?;

        // Fill in training metadata
        result.br_training_steps = br_steps;
        result.br_training_time_secs = br_time;
        result.br_still_learning = still_learning;

        println!(
            "  ✓ BR Advantage: {:.4} | BR win rate: {:.1}% | BR avg: {:.2} | Victim avg: {:.2}",
            result.br_advantage,
            result.br_win_rate * 100.0,
            result.br_avg_return,
            result.victim_avg_return
        );

        results.push(result);
        println!();
    }

    // Save results and generate graphs
    println!("✓ Evaluation complete for {} checkpoint(s)", results.len());

    // Determine output paths
    let run_folder = if args.path.join("checkpoints").is_dir() {
        args.path.clone()
    } else {
        args.path
            .parent()
            .context("Failed to get parent directory of checkpoint path")?
            .to_path_buf()
    };

    let json_path = if let Some(ref output) = args.output {
        output.clone()
    } else {
        run_folder.join("exploitability_results.json")
    };

    let graph_path = run_folder.join("exploitability_over_training.png");

    // Save JSON
    save_results_json(&results, &config, E::NAME, &json_path)?;
    println!("✓ Saved results to: {}", json_path.display());

    // Generate graph
    generate_graph(&results, &graph_path)?;
    println!("✓ Saved graph to: {}", graph_path.display());

    Ok(())
}
